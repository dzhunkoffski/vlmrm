{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты по статье"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для установки кода, достаточно следовать описанию в ридми, в целом мне удалось запустить код и без докера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяю, что агент выучивается + как различаются ground truth reward и vlm reward. авторы не предоставили детального описания или конфигов для воспроизведения я буду использовать часть гиперпараметров отсюда: https://huggingface.co/sb3/sac-MountainCarContinuous-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing exp1.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile exp1.yaml\n",
    "\n",
    "env_name: MountainCarContinuous-v0 # RL environment name\n",
    "base_path: runs/training # Base path to save logs and checkpoints seed: 42 # Seed for reproducibility\n",
    "description: MountainCar using CLIP reward\n",
    "tags: # Wandb tags\n",
    "    - training\n",
    "    - mountaincar\n",
    "    - CLIP\n",
    "reward:\n",
    "    name: clip\n",
    "    pretrained_model: RN50/openai # CLIP\n",
    "    # CLIP batch size per synchronous inference step.\n",
    "    # # Batch size must be divisible by n_workers (GPU count)\n",
    "    # # so that it can be shared among workers, and must be a divisor\n",
    "    # # of n_envs * episode_length so that all batches can be of the\n",
    "    # # same size (no support for variable batch size as of now.)\n",
    "    batch_size: 1600\n",
    "    alpha: 0.5 # Alpha value of Baseline CLIP (CO-RELATE)\n",
    "    target_prompts: # Description of the goal state\n",
    "        - a car at the peak of the mountain, next to the yellow flag\n",
    "    baseline_prompts: # Description of the environment\n",
    "        - a car in the mountain\n",
    "    # Path to pre-saved model weights. When executing multiple runs,\n",
    "    # # mount a volume to this path to avoid downloading the model\n",
    "    # # weights multiple times.\n",
    "    cache_dir: root/.cache\n",
    "rl:\n",
    "    policy_name: MlpPolicy\n",
    "    policy_kwargs:\n",
    "        log_std_init: -3.67\n",
    "        net_arch: \n",
    "            - 64\n",
    "            - 64\n",
    "        n_steps: 150000 # Total number of simulation steps to be collected.\n",
    "        n_envs_per_worker: 8 # Number of environments per worker (GPU)\n",
    "        episode_length: 200 # Desired episode length\n",
    "        learning_starts: 0 # Number of env steps to collect before training\n",
    "        train_freq: 200 # (эта штука должна делиться на episode_length\n",
    "        batch_size: 256 # SAC buffer sample size per gradient step\n",
    "        gradient_steps: 50 # Number of samples to collect from the buffer per training step\n",
    "        tau: 0.01 # SAC target network update rate\n",
    "        gamma: 0.9999 # SAC discount factor\n",
    "        learning_rate: 1e-3 # SAC optimizer learning rate\n",
    "    logging:\n",
    "        checkpoint_freq: 800 # Number of env steps between checkpoints\n",
    "        video_freq: 800 # Number of env steps between videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 vlmrm train \"$(cat exp1.yaml)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посути предыдущий резльутат не завелся (агента не удалось обучить чтобы он ездил к желтому флагу). Пробую проверить, работает ли вообще авторский код без CLIP reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing exp_baseline.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile exp_baseline.yaml\n",
    "\n",
    "env_name: MountainCarContinuous-v0 # RL environment name\n",
    "base_path: runs/training # Base path to save logs and checkpoints\n",
    "seed: 42 # Seed for reproducibility\n",
    "description: MountainCar using CLIP reward\n",
    "tags: # Wandb tags\n",
    "    - training\n",
    "    - mountaincar\n",
    "    - gt \n",
    "reward:\n",
    "    name: ground_truth\n",
    "    batch_size: 1\n",
    "    alpha: 0.5\n",
    "    cache_dir: root/.cache\n",
    "rl:\n",
    "    policy_name: MlpPolicy\n",
    "    policy_kwargs: \n",
    "        log_std_init: -3.67\n",
    "        net_arch:\n",
    "            - 64\n",
    "            - 64\n",
    "    n_steps: 3000000 # Total number of simulation steps to be collected.\n",
    "    n_envs_per_worker: 8 # Number of environments per worker (GPU)\n",
    "    episode_length: 200 # Desired episode length\n",
    "    learning_starts: 0 # Number of env steps to collect before training\n",
    "    train_freq: 200 # (эта штука должна делиться на episode_length\n",
    "    batch_size: 256 # SAC buffer sample size per gradient step\n",
    "    gradient_steps: 50 # Number of samples to collect from the buffer per training step\n",
    "    tau: 0.01 # SAC target network update rate\n",
    "    gamma: 0.9999 # SAC discount factor\n",
    "    learning_rate: 1e-3 # SAC optimizer learning rate\n",
    "logging:\n",
    "    tensorboard_freq: 800\n",
    "    checkpoint_freq: 800 # Number of env steps between checkpoints\n",
    "    video_freq: 800 # Number of env steps between videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 vlmrm train \"$(cat exp_baseline.yaml)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как эксперимент 1, но пробую большую CLIP модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing exp2.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile exp2.yaml\n",
    "\n",
    "env_name: MountainCarContinuous-v0 # RL environment name\n",
    "base_path: runs/training # Base path to save logs and checkpoints\n",
    "seed: 42 # Seed for reproducibility\n",
    "description: MountainCar using CLIP reward\n",
    "tags: # Wandb tags\n",
    "    - training\n",
    "    - mountaincar\n",
    "    - CLIP \n",
    "reward: \n",
    "    name: clip\n",
    "    pretrained_model: ViT-g-14/laion2b_s34b_b88k # CLIP # CLIP batch size per synchronous inference step. # Batch size must be divisible by n_workers (GPU count) # so that it can be shared among workers, and must be a divisor # of n_envs * episode_length so that all batches can be of the # same size (no support for variable batch size as of now.)\n",
    "    batch_size: 1600\n",
    "    alpha: 0.8 # Alpha value of Baseline CLIP (CO-RELATE)\n",
    "    target_prompts: # Description of the goal state \n",
    "        - a car at the peak of the mountain, next to the yellow flag \n",
    "    baseline_prompts: # Description of the environment \n",
    "        - a car in the mountain # Path to pre-saved model weights. When executing multiple runs, # mount a volume to this path to avoid downloading the model # weights multiple times. \n",
    "    cache_dir: root/.cache \n",
    "rl:\n",
    "    policy_name: MlpPolicy \n",
    "    policy_kwargs: \n",
    "        net_arch: \n",
    "            pi: [64, 64] \n",
    "            qf: [64, 64] \n",
    "    n_steps: 1000000 # Total number of simulation steps to be collected. \n",
    "    n_envs_per_worker: 4 # Number of environments per worker (GPU) \n",
    "    episode_length: 400 # Desired episode length \n",
    "    learning_starts: 75000 # Number of env steps to collect before training \n",
    "    train_freq: 400 # (эта штука должна делиться на episode_length \n",
    "    batch_size: 64 # SAC buffer sample size per gradient step \n",
    "    gradient_steps: 4 # Number of samples to collect from the buffer per training step \n",
    "    tau: 0.01 # SAC target network update rate \n",
    "    gamma: 0.9999 # SAC discount factor \n",
    "    learning_rate: 1e-4 # SAC optimizer learning rate \n",
    "    ent_coef: \"auto\" \n",
    "    target_entropy: 0.1 \n",
    "    buffer_size: 500000 \n",
    "    logging: \n",
    "        checkpoint_freq: 800\n",
    "        video_freq: 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 vlmrm train \"$(cat exp2.yaml)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlmrm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
