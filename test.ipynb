{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты по статье"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперимент 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяю авторское решение для задачи `MountainCar`, хочу удостовериться, что ground truth reward и vlm reward скорелированы. К сожалению, авторы не предоставили детального описания или конфигов для воспроизведения я буду использовать часть гиперпараметров отсюда: https://huggingface.co/sb3/sac-MountainCarContinuous-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config_exp7.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config_exp7.yaml\n",
    "\n",
    "env_name: MountainCarContinuous-v0 # RL environment name\n",
    "base_path: runs/training # Base path to save logs and checkpoints\n",
    "seed: 42 # Seed for reproducibility\n",
    "description: MountainCar using CLIP reward\n",
    "tags: # Wandb tags\n",
    "  - training\n",
    "  - mountaincar\n",
    "  - CLIP\n",
    "reward:\n",
    "  name: clip\n",
    "  pretrained_model: ViT-g-14/laion2b_s34b_b88k  # CLIP \n",
    "  # CLIP batch size per synchronous inference step.\n",
    "  # Batch size must be divisible by n_workers (GPU count)\n",
    "  # so that it can be shared among workers, and must be a divisor\n",
    "  # of n_envs * episode_length so that all batches can be of the\n",
    "  # same size (no support for variable batch size as of now.)\n",
    "  batch_size: 1600\n",
    "  alpha: 0.8 # Alpha value of Baseline CLIP (CO-RELATE)\n",
    "  target_prompts: # Description of the goal state\n",
    "    - a car at the peak of the mountain, next to the yellow flag\n",
    "  baseline_prompts: # Description of the environment\n",
    "    - a car in the mountain\n",
    "  # Path to pre-saved model weights. When executing multiple runs,\n",
    "  # mount a volume to this path to avoid downloading the model\n",
    "  # weights multiple times.\n",
    "  cache_dir: root/.cache\n",
    "rl:\n",
    "  policy_name: MlpPolicy\n",
    "  policy_kwargs:\n",
    "    net_arch:\n",
    "      pi: [64, 64]\n",
    "      qf: [64, 64]\n",
    "  n_steps: 1000000 # Total number of simulation steps to be collected.\n",
    "  n_envs_per_worker: 4 # Number of environments per worker (GPU)\n",
    "  episode_length: 400 # Desired episode length\n",
    "  learning_starts: 75000 # Number of env steps to collect before training\n",
    "  train_freq: 400 # (эта штука должна делиться на episode_length\n",
    "  batch_size: 64 # SAC buffer sample size per gradient step\n",
    "  gradient_steps: 4 # Number of samples to collect from the buffer per training step\n",
    "  tau: 0.01 # SAC target network update rate\n",
    "  gamma: 0.9999 # SAC discount factor\n",
    "  learning_rate: 1e-4 # SAC optimizer learning rate\n",
    "  ent_coef: \"auto\"\n",
    "  target_entropy: 0.1\n",
    "  buffer_size: 500000\n",
    "logging:\n",
    "  checkpoint_freq: 800 # Number of env steps between checkpoints\n",
    "  video_freq: 800 # Number of env steps between videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stud_lab_vk_01/miniconda3/envs/vlmrm/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/stud_lab_vk_01/miniconda3/envs/vlmrm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_checkpoint\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/stud_lab_vk_01/miniconda3/envs/vlmrm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_base_path\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2025-03-29 20:17:21.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlmrm.trainer.train\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m146\u001b[0m - \u001b[1mCommand called: /home/stud_lab_vk_01/miniconda3/envs/vlmrm/bin/vlmrm train \n",
      "env_name: MountainCarContinuous-v0 # RL environment name\n",
      "base_path: runs/training # Base path to save logs and checkpoints\n",
      "seed: 42 # Seed for reproducibility\n",
      "description: MountainCar using CLIP reward\n",
      "tags: # Wandb tags\n",
      "  - training\n",
      "  - mountaincar\n",
      "  - CLIP\n",
      "reward:\n",
      "  name: clip\n",
      "  pretrained_model: RN50/openai # CLIP \n",
      "  # CLIP batch size per synchronous inference step.\n",
      "  # Batch size must be divisible by n_workers (GPU count)\n",
      "  # so that it can be shared among workers, and must be a divisor\n",
      "  # of n_envs * episode_length so that all batches can be of the\n",
      "  # same size (no support for variable batch size as of now.)\n",
      "  batch_size: 1600\n",
      "  alpha: 0.5 # Alpha value of Baseline CLIP (CO-RELATE)\n",
      "  target_prompts: # Description of the goal state\n",
      "    - a car at the peak of the mountain, nearby the yellow flag\n",
      "  baseline_prompts: # Description of the environment\n",
      "    - a car in the mountain\n",
      "  # Path to pre-saved model weights. When executing multiple runs,\n",
      "  # mount a volume to this path to avoid downloading the model\n",
      "  # weights multiple times.\n",
      "  cache_dir: root/.cache\n",
      "rl:\n",
      "  policy_name: MlpPolicy\n",
      "  policy_kwargs:\n",
      "    log_std_init: -3.67\n",
      "    net_arch:\n",
      "      - 64\n",
      "      - 64\n",
      "  n_steps: 150000 # Total number of simulation steps to be collected.\n",
      "  n_envs_per_worker: 8 # Number of environments per worker (GPU)\n",
      "  episode_length: 200 # Desired episode length\n",
      "  learning_starts: 0 # Number of env steps to collect before training\n",
      "  train_freq: 200 # (эта штука должна делиться на episode_length\n",
      "  batch_size: 256 # SAC buffer sample size per gradient step\n",
      "  gradient_steps: 50 # Number of samples to collect from the buffer per training step\n",
      "  tau: 0.01 # SAC target network update rate\n",
      "  gamma: 0.9999 # SAC discount factor\n",
      "  learning_rate: 1e-3 # SAC optimizer learning rate\n",
      "logging:\n",
      "  checkpoint_freq: 800 # Number of env steps between checkpoints\n",
      "  video_freq: 800 # Number of env steps between videos\u001b[0m\n",
      "\u001b[32m2025-03-29 20:17:21.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlmrm.trainer.train\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mStarted run with id MountainCarContinuous_CLIP_2025-03-29_20-17-21_3b9158e1\u001b[0m\n",
      "\u001b[32m2025-03-29 20:17:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlmrm.trainer.train\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mLogging experiment metadata\u001b[0m\n",
      "\u001b[32m2025-03-29 20:17:21.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlmrm.trainer.train\u001b[0m:\u001b[36m_train\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mRunning CLIP-rewarded SAC. Spawning workers.\u001b[0m\n",
      "/home/stud_lab_vk_01/miniconda3/envs/vlmrm/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/stud_lab_vk_01/miniconda3/envs/vlmrm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_checkpoint\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/stud_lab_vk_01/miniconda3/envs/vlmrm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_base_path\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2025-03-29 20:17:24.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvlmrm.trainer.train\u001b[0m:\u001b[36mprimary_worker\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mCreating environment instance\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/stud_lab_vk_01/miniconda3/envs/vlmrm/lib/python3.9/multiprocessing/forkserver.py\", line 258, in main\n",
      "    fds = reduction.recvfds(s, MAXFDS_TO_SEND + 1)\n",
      "  File \"/home/stud_lab_vk_01/miniconda3/envs/vlmrm/lib/python3.9/multiprocessing/reduction.py\", line 159, in recvfds\n",
      "    raise EOFError\n",
      "EOFError\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 vlmrm train \"$(cat config_exp2.yaml)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlmrm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
